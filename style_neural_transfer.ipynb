{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <u><b>Neural Style Transfer:</b></u>\n",
        "<h3> -------- Author- Amar Choudhary ----------- </h3>\n",
        "\n",
        "## Introduction\n",
        "In this project , we make or transform a new image which has features of both content and style image from both content and style image that we pass in model. \n",
        "\n",
        "For this we have to do fine tunning on vgg19 model and calculate both content loss and style loss.\n",
        "\n",
        "![Flow Diagram](https://www.researchgate.net/publication/347180236/figure/fig3/AS:1022415605886976@1620774284899/Model-Optimisation-Based-Neural-Style-Transfer-System-5.jpg)\n",
        "\n",
        "\n",
        "## *So we got Total Loss :*\n",
        "\n",
        "*     Total Loss = Content Loss + Style Loss\n",
        "\n",
        "\n",
        "## The process involves three key steps:\n",
        "\n",
        "* *Content Extraction:* A content image is processed through the CNN to capture its structure and essential details.\n",
        "\n",
        "* *Style Extraction:* A style image is passed through the same network to extract style features, including textures and colors, using Gram matrices from different layers.\n",
        "\n",
        "* *Optimization:* Starting with a copy of the content image, an iterative optimization process adjusts this image to minimize the loss function. This loss combines content loss (difference from the original content image) and style loss (difference from the style image).\n",
        "\n",
        "## <i>We got final Results as :</i>\n",
        "\n",
        "\n",
        "![3](../markdown/3.png)\n",
        "\n",
        "![7](https://github.com/AmarBackInField/NeuralStyleTransfer-v1.0/assets/126746349/3803131e-996f-4f5f-b3b1-f4d2c213a76a)\n",
        "\n",
        "![9](https://github.com/AmarBackInField/NeuralStyleTransfer-v1.0/assets/126746349/9dc3fc70-7a7b-466f-be36-fdbc101b19c7)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IuOYRJbXgxr"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5oKfMYhOrnJ8"
      },
      "outputs": [],
      "source": [
        "# -- author --- Amar Choudhary\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras.preprocessing.image as process_im\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.applications import VGG19\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras import losses\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras import backend as K\n",
        "import functools\n",
        "import IPython.display\n",
        "import os\n",
        "import cv2\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F-LzznPYrIh"
      },
      "source": [
        "# Downloading Dataset from KAGGLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvfwMLcdrxX3",
        "outputId": "dafb73f1-0e5d-41ca-dd4c-34b6a42fa727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG1aJ-R3utsF",
        "outputId": "252105cf-02a6-40f2-e6f7-6cce9ad0d520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/ikarus777/best-artworks-of-all-time\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading best-artworks-of-all-time.zip to /content\n",
            "100% 2.29G/2.29G [00:31<00:00, 133MB/s]\n",
            "100% 2.29G/2.29G [00:31<00:00, 77.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d ikarus777/best-artworks-of-all-time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lE_1kUPuvRd",
        "outputId": "1d5b7fd9-82d5-42bb-d08b-86c7b6812ca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/duttadebadri/image-classification\n",
            "License(s): CC0-1.0\n",
            "image-classification.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d duttadebadri/image-classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag3jPjcaYj8S"
      },
      "source": [
        "# Unzipping All Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqdyAOGmuyXc",
        "outputId": "0ff41589-8a95-43bc-8c21-272914e8b94e"
      },
      "outputs": [],
      "source": [
        "# Unzipping All Data (content + Style)\n",
        "!unzip best-artworks-of-all-time.zip\n",
        "!unzip image-classification.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HowF_UbOu0PB"
      },
      "outputs": [],
      "source": [
        "# extracting artists names\n",
        "list_of_artists=os.listdir(\"images/images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp7s5F4yu4Pz",
        "outputId": "5524de9e-3ce0-4ae1-935e-3bdbfd2f0da6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55\n",
            "['Gustav_Klimt', 'Gustave_Courbet', 'Pierre-Auguste_Renoir', 'Henri_de_Toulouse-Lautrec', 'Peter_Paul_Rubens', 'Caravaggio', 'Francisco_Goya', 'Edouard_Manet', 'Albrecht_DuтХа├кrer', 'Mikhail_Vrubel', 'Vincent_van_Gogh', 'Titian', 'Edgar_Degas', 'food and d rinks', 'Frida_Kahlo', 'Sandro_Botticelli', 'Henri_Rousseau', 'Eugene_Delacroix', 'Michelangelo', 'Raphael', 'Vasiliy_Kandinskiy', 'Albrecht_Du╠Иrer', 'Andrei_Rublev', 'Claude_Monet', 'Giotto_di_Bondone', 'Leonardo_da_Vinci', 'Hieronymus_Bosch', 'Camille_Pissarro', 'Rembrandt', 'Marc_Chagall', 'Andy_Warhol', 'Paul_Cezanne', 'William_Turner', 'Paul_Klee', 'Diego_Velazquez', 'art and culture', 'Jackson_Pollock', 'Joan_Miro', 'Pieter_Bruegel', 'Kazimir_Malevich', 'Henri_Matisse', 'Pablo_Picasso', 'Alfred_Sisley', 'Diego_Rivera', 'Georges_Seurat', 'Piet_Mondrian', 'Amedeo_Modigliani', 'Salvador_Dali', 'Paul_Gauguin', 'El_Greco', 'Jan_van_Eyck', 'architecure', 'Edvard_Munch', 'travel and  adventure', 'Rene_Magritte']\n"
          ]
        }
      ],
      "source": [
        "print(len(list_of_artists)) # NUMBER OF ARTISTS\n",
        "print(list_of_artists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "O3cBDdObu6L_"
      },
      "outputs": [],
      "source": [
        "def img_by_artist(path_dir):\n",
        "  list_of_artists=os.listdir(\"images/images\") # 55 artists list\n",
        "  fig,axs=plt.subplots(5,5,figsize=(30,30)) # PLOTING IN 5 ROWS AND 5 COLUMNS\n",
        "  # plt.figure(figsize=(50,50))\n",
        "  for i in range(5):\n",
        "    artist=random.choice(list_of_artists) # Picking_up one artist\n",
        "    path=os.path.join(path_dir,artist)\n",
        "    path_list=os.listdir(path) # Corresponding to that artist it no. of painting path\n",
        "    for j in range(5):\n",
        "      path_of_image=random.choice(path_list)\n",
        "      full_path_dir=os.path.join(\"images/images\",artist,path_of_image)\n",
        "      img_arr=cv2.imread(full_path_dir)\n",
        "      img_arr=cv2.resize(img_arr,(256,256))\n",
        "      axs[i,j].imshow(img_arr)\n",
        "      plt.title(artist)\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "T67mh3DbFP5F"
      },
      "outputs": [],
      "source": [
        "def show_im(img,title=None):\n",
        "    img=np.squeeze(img,axis=0) #squeeze array to drop batch axis\n",
        "    plt.imshow(np.uint8(img))\n",
        "    if title is None:\n",
        "        pass\n",
        "    else:\n",
        "        plt.title(title)\n",
        "    plt.imshow(np.uint8(img))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FNBboeSEu9WF",
        "outputId": "a4236f92-14b4-4c3e-e686-49100ccb0f96"
      },
      "outputs": [],
      "source": [
        "img_by_artist(\"images/images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![output](markdown\\1.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Y1Py_qsTu_RK"
      },
      "outputs": [],
      "source": [
        "def load_file(image_path):\n",
        "    image =  Image.open(image_path)\n",
        "    max_dim=512\n",
        "    factor=max_dim/max(image.size)\n",
        "    image=image.resize((round(image.size[0]*factor),round(image.size[1]*factor)),Image.ANTIALIAS)\n",
        "    im_array = process_im.img_to_array(image)\n",
        "    im_array = np.expand_dims(im_array,axis=0)\n",
        "    return im_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "xcB_pEx5wXAY"
      },
      "outputs": [],
      "source": [
        "content_path=\"validation/validation/art and culture/0002.jpg\"\n",
        "style_path=\"images/images/Alfred_Sisley/Alfred_Sisley_10.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfZmcNrawz7E",
        "outputId": "9854c7bd-f2c1-4985-ee59-ab6daaf24159"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-66992c8f8f0a>:5: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  image=image.resize((round(image.size[0]*factor),round(image.size[1]*factor)),Image.ANTIALIAS)\n"
          ]
        }
      ],
      "source": [
        "content = load_file(content_path)\n",
        "style = load_file(style_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTjYIsT-w3EF"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "content = load_file(content_path)\n",
        "style = load_file(style_path)\n",
        "plt.subplot(1,2,1)\n",
        "show_im(content,'Content Image')\n",
        "plt.subplot(1,2,2)\n",
        "show_im(style,'Style Image')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZGspgebxR7T"
      },
      "source": [
        "# Declaring Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hdKAEMdxOAw"
      },
      "outputs": [],
      "source": [
        "def img_preprocess(img_path):\n",
        "    image=load_file(img_path) # Converting into image array\n",
        "    img=tf.keras.applications.vgg19.preprocess_input(image)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1hLB0LIxeyS"
      },
      "outputs": [],
      "source": [
        "def deprocess_img(processed_img):\n",
        "  x = processed_img.copy()\n",
        "  if len(x.shape) == 4:\n",
        "    x = np.squeeze(x, 0)\n",
        "  assert len(x.shape) == 3 #Input dimension must be [1, height, width, channel] or [height, width, channel]\n",
        "\n",
        "\n",
        "  # perform the inverse of the preprocessing step\n",
        "  x[:, :, 0] += 103.939\n",
        "  x[:, :, 1] += 116.779\n",
        "  x[:, :, 2] += 123.68\n",
        "  x = x[:, :, ::-1] # converting BGR to RGB channel\n",
        "\n",
        "  x = np.clip(x, 0, 255).astype('uint8')\n",
        "  return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQUsdYAMXgxy"
      },
      "source": [
        "![Flow Diagram](https://www.researchgate.net/publication/347180236/figure/fig3/AS:1022415605886976@1620774284899/Model-Optimisation-Based-Neural-Style-Transfer-System-5.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLnjdNScxuMS"
      },
      "outputs": [],
      "source": [
        "content_layers = ['block5_conv2']\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1',\n",
        "                'block4_conv1',\n",
        "                'block5_conv1']\n",
        "number_content=len(content_layers)\n",
        "number_style =len(style_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EikNHjZGxojK"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "    vgg=tf.keras.applications.vgg19.VGG19(include_top=False,weights='imagenet')\n",
        "    vgg.trainable=False\n",
        "    content_output=[vgg.get_layer(layer).output for layer in content_layers]\n",
        "    style_output=[vgg.get_layer(layer).output for layer in style_layers]\n",
        "    model_output= style_output+content_output\n",
        "    return models.Model(vgg.input,model_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fayRAm8myA6Q"
      },
      "source": [
        "# Layers of VGG19 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdSqQQsexyjx"
      },
      "outputs": [],
      "source": [
        "model=VGG19(include_top=False,weights='imagenet')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5L8MGeRyI4y"
      },
      "outputs": [],
      "source": [
        "for layer in model.layers:\n",
        "  print(layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu4A1-VjyWGS"
      },
      "source": [
        "# Model with having Content and Style Layers only\n",
        "* content_layers =\n",
        "                ['block5_conv2']\n",
        "* style_layers =\n",
        "                ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1',\n",
        "                'block4_conv1',\n",
        "                'block5_conv1']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQNpljI-yO3E"
      },
      "outputs": [],
      "source": [
        "model=get_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hg-JpFf0Bi0"
      },
      "source": [
        "# Defining Content and style loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0ByKhZf1lgK"
      },
      "source": [
        "![Coontent Loss](https://cdn-images-1.medium.com/max/800/1*1YfGhmzBw7EK3e8CRpZbuA.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akOHmgLq0HuU"
      },
      "outputs": [],
      "source": [
        "def get_content_loss(noise,target):\n",
        "    loss = tf.reduce_mean(tf.square(noise-target))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sv8mG3D1rSS"
      },
      "source": [
        "![Gram Matrix](https://cdn-images-1.medium.com/max/800/1*5xx9KmhVb59Mxe_buOwHBA.png)\n",
        "\n",
        "![Style loss](https://cdn-images-1.medium.com/max/800/1*PuYveCM2BlgFfjUCr6I_Ng.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "u83VFYVd1m_W"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(tensor):\n",
        "    channels=int(tensor.shape[-1])\n",
        "    vector=tf.reshape(tensor,[-1,channels])\n",
        "    n=tf.shape(vector)[0]\n",
        "    gram_matrix=tf.matmul(vector,vector,transpose_a=True)\n",
        "    return gram_matrix/tf.cast(n,tf.float32)\n",
        "\n",
        "def get_style_loss(noise,target):\n",
        "    gram_noise=gram_matrix(noise)\n",
        "    #gram_target=gram_matrix(target)\n",
        "    loss=tf.reduce_mean(tf.square(target-gram_noise))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "bZrNFChz1qwM"
      },
      "outputs": [],
      "source": [
        "def get_features(model,content_path,style_path):\n",
        "    content_img=img_preprocess(content_path)\n",
        "    style_image=img_preprocess(style_path)\n",
        "\n",
        "    content_output=model(content_img)\n",
        "    style_output=model(style_image)\n",
        "\n",
        "    content_feature = [layer[0] for layer in content_output[number_style:]]\n",
        "    style_feature = [layer[0] for layer in style_output[:number_style]]\n",
        "    return content_feature,style_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "q9RLJ_Gd11yP"
      },
      "outputs": [],
      "source": [
        "def compute_loss(model, loss_weights,image, gram_style_features, content_features):\n",
        "    style_weight,content_weight = loss_weights #style weight and content weight are user given parameters\n",
        "                                               #that define what percentage of content and/or style will be preserved in the generated image\n",
        "\n",
        "    output=model(image)\n",
        "    content_loss=0\n",
        "    style_loss=0\n",
        "\n",
        "    noise_style_features = output[:number_style]\n",
        "    noise_content_feature = output[number_style:]\n",
        "\n",
        "    weight_per_layer = 1.0/float(number_style)\n",
        "    for a,b in zip(gram_style_features,noise_style_features):\n",
        "        style_loss+=weight_per_layer*get_style_loss(b[0],a)\n",
        "\n",
        "\n",
        "    weight_per_layer =1.0/ float(number_content)\n",
        "    for a,b in zip(noise_content_feature,content_features):\n",
        "        content_loss+=weight_per_layer*get_content_loss(a[0],b)\n",
        "\n",
        "    style_loss *= style_weight\n",
        "    content_loss *= content_weight\n",
        "\n",
        "    total_loss = content_loss + style_loss\n",
        "\n",
        "\n",
        "    return total_loss,style_loss,content_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Gi2gePhg14SB"
      },
      "outputs": [],
      "source": [
        "def compute_grads(dictionary):\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_loss=compute_loss(**dictionary)\n",
        "\n",
        "    total_loss=all_loss[0]\n",
        "    return tape.gradient(total_loss,dictionary['image']),all_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "zpm51GHM18V6"
      },
      "outputs": [],
      "source": [
        "def run_style_transfer(content_path,style_path,epochs=500,content_weight=1e3, style_weight=1e-2):\n",
        "\n",
        "    model=get_model()\n",
        "\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    content_feature,style_feature = get_features(model,content_path,style_path)\n",
        "    style_gram_matrix=[gram_matrix(feature) for feature in style_feature]\n",
        "\n",
        "    noise = img_preprocess(content_path)\n",
        "    noise=tf.Variable(noise,dtype=tf.float32)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n",
        "\n",
        "    best_loss,best_img=float('inf'),None\n",
        "\n",
        "    loss_weights = (style_weight, content_weight)\n",
        "    dictionary={'model':model,\n",
        "              'loss_weights':loss_weights,\n",
        "              'image':noise,\n",
        "              'gram_style_features':style_gram_matrix,\n",
        "              'content_features':content_feature}\n",
        "\n",
        "    norm_means = np.array([103.939, 116.779, 123.68])\n",
        "    min_vals = -norm_means\n",
        "    max_vals = 255 - norm_means\n",
        "\n",
        "    imgs = []\n",
        "    for i in range(epochs):\n",
        "        grad,all_loss=compute_grads(dictionary)\n",
        "        total_loss,style_loss,content_loss=all_loss\n",
        "        optimizer.apply_gradients([(grad,noise)])\n",
        "        clipped=tf.clip_by_value(noise,min_vals,max_vals)\n",
        "        noise.assign(clipped)\n",
        "\n",
        "        if total_loss<best_loss:\n",
        "            best_loss = total_loss\n",
        "            best_img = deprocess_img(noise.numpy())\n",
        "\n",
        "         #for visualization\n",
        "\n",
        "    #     if i%5==0:\n",
        "    #         plot_img = noise.numpy()\n",
        "    #         plot_img = deprocess_img(plot_img)\n",
        "    #         imgs.append(plot_img)\n",
        "    #         IPython.display.clear_output(wait=True)\n",
        "    #         IPython.display.display_png(Image.fromarray(plot_img))\n",
        "    #         print('Epoch: {}'.format(i))\n",
        "    #         print('Total loss: {:.4e}, '\n",
        "    #           'style loss: {:.4e}, '\n",
        "    #           'content loss: {:.4e}, '.format(total_loss, style_loss, content_loss))\n",
        "\n",
        "    # IPython.display.clear_output(wait=True)\n",
        "\n",
        "\n",
        "    return best_img,best_loss,imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYPjVFRpXgx0",
        "outputId": "efb12fdc-977d-4ad8-ea58-2bc7b1579367"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-66992c8f8f0a>:5: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  image=image.resize((round(image.size[0]*factor),round(image.size[1]*factor)),Image.ANTIALIAS)\n"
          ]
        }
      ],
      "source": [
        "best, best_loss,image = run_style_transfer(content_path,style_path, epochs=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "Y_Gfs2QX2DFA",
        "outputId": "3c29cf9c-c8d9-4516-d4de-78ffacef90bf"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(best)\n",
        "plt.title('Style transfer Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.subplot(1,3,1)\n",
        "show_im(content,'Content Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.subplot(1,3,2)\n",
        "show_im(style,'Style Image')\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![output1](markdown\\2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p6X8G_CMmu6",
        "outputId": "29d79842-069c-4ed9-a375-956eaa11d2c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 512, 403, 3)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "content.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "oEPZVt5VF1hg"
      },
      "outputs": [],
      "source": [
        "# Here content , style are path in string format\n",
        "def plot_result(best,content,style):\n",
        "  plt.figure(figsize=(15,15))\n",
        "  plt.subplot(1,3,3)\n",
        "  plt.imshow(best)\n",
        "  plt.title('Style transfer Image')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.subplot(1,3,1)\n",
        "  show_im(content,'Content Image')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.subplot(1,3,2)\n",
        "  show_im(style,'Style Image')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "MIGjJIRaHDMf"
      },
      "outputs": [],
      "source": [
        "def find_result():\n",
        "  artist=random.choice(os.listdir(\"images/images\"))\n",
        "  path=os.path.join(\"images/images\",artist)\n",
        "  items=os.listdir(path) # In list format\n",
        "\n",
        "  category=random.choice(os.listdir(\"validation/validation\"))\n",
        "  path1=os.path.join(\"validation/validation\",category)\n",
        "  items1=os.listdir(path1)\n",
        "  for i in range(5):\n",
        "    choose_one_item_at_time_in_style=random.choice(items)\n",
        "    choose_one_item_At_time_in_style_dir=os.path.join(path,choose_one_item_at_time_in_style)\n",
        "    style_path=choose_one_item_At_time_in_style_dir # style path\n",
        "    style=load_file(style_path)\n",
        "    choose_one_item_at_time_in_content=random.choice(items1)\n",
        "    choose_one_item_At_time_in_content_dir=os.path.join(path1,choose_one_item_at_time_in_content)\n",
        "    content_path=choose_one_item_At_time_in_content_dir # content path\n",
        "    content=load_file(content_path)\n",
        "    best, best_loss,image = run_style_transfer(content_path,style_path, epochs=500)\n",
        "    plot_result(best,content,style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K6jiHyAfJeHm",
        "outputId": "b91b75e0-b711-4228-eaac-34989673bf42"
      },
      "outputs": [],
      "source": [
        "find_result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![output1](markdown\\2.png)\n",
        "![output1](markdown\\3.png)\n",
        "![output1](markdown\\5.png)\n",
        "![output1](markdown\\7.png)\n",
        "![output1](markdown\\8.png)\n",
        "![output1](markdown\\9.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj5vbGzrXgx3"
      },
      "source": [
        "# Resource"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rXVngCFXgx4"
      },
      "source": [
        "* For Dataset - https://www.kaggle.com/datasets\n",
        "* Taking Refrence Official Papers - https://arxiv.org/pdf/1508.06576.pdf https://arxiv.org/pdf/1603.08155.pdf\n",
        "* https://towardsdatascience.com/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQJCghTJXgx4"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
